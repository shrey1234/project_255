# -*- coding: utf-8 -*-
"""email_classification_copy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ckV7zTkhM8PxZXlCVVauv9FBaM_aH5sT

Dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
#  % pip install polyglot
#  % pip install pyicu
#  % pip install pycld2
#  % pip install nltk
#  % pip install tldextract


import nltk
nltk.download('punkt')
from nltk import word_tokenize 
import email
import base64
import os
import re 
import csv
from difflib import SequenceMatcher
import pandas as pd
import quopri as qp
from bs4 import BeautifulSoup
from email.iterators import _structure
from polyglot.detect import Detector
from urllib.parse import urlparse
import tldextract

"""Mounting Drive for data"""

from google.colab import drive
drive.mount('/content/drive')

"""Functions to parse header,body and url"""

def parse_url(urls):
    
    ip_address=[];
    len_url=[];
    special_sign_url=[]
    pos_url=[]
    final_url=[]
    subdomain_url=[]
    mail_url = []
    resolve_url = []

    for url in urls:
        #print("Url: ",url)
        #domain of url has ip or not
        try:
            domain =urlparse(url).netloc
            if (domain.replace('.', '').replace(':','').isnumeric()):
                cur_ip=True
            else:
                cur_ip=False
        except:
            print("error occured while parsing domain")
            cur_ip=False
        
        ip_address.append(cur_ip)    
        
        #length of url is 75 or not
        if(len(url)>85):
            cur_len=True
        else:
            cur_len=False
        len_url.append(cur_len)    
        
        #url conatins @ or not
        if "@"  in url: 
            cur_special=True
        else:
            cur_special=False
        special_sign_url.append(cur_special)    
            
        #position of // in url    
        cur_pos=False    
        if "//" in url:    
            res= url.rindex("//") 
            if(res > 7):
                cur_pos=True
            else:
                cur_pos=False  
        pos_url.append(cur_pos)   
        
        #subdomain and multidomain
        domain1=tldextract.extract(url)
        domain1=domain1.domain
        cur_subdomain=False
        if '-' in domain1:
            cur_subdomain=True
        subdomain_url.append(cur_subdomain) 
        
         #mailto function in url
        if "mailto:" in url:
            cur_mail=True
        else:
            cur_mail=False
        mail_url.append(cur_mail) 

         #domain resolve
        try:
            answers = dns.resolver.resolve(domain,'NS')
            cur_resolve=False
        except:
            cur_resolve=True 
    
        resolve_url.append(cur_resolve)
        
    final_url.append(any(ip_address))
    final_url.append(any(len_url))
    final_url.append(any(special_sign_url))
    final_url.append(any(pos_url))
    final_url.append(any(subdomain_url))
    final_url.append(any(mail_url))
    final_url.append(any(resolve_url))

    return final_url

def clean_text(body, debug=False):
    #removing url
    
    #removing urls in text
    body = re.sub('http[s]?://\S+', '', body)
    if debug:
      print(body) 
    #removing punctuations
    body = re.sub(r'[^\w\s]', '', body)
    if debug:
      print(body)
    #removing non latin characters
    body = re.sub(r'[^\x00-\x7F\x80-\xFF\u0100-\u017F\u0180-\u024F\u1E00-\u1EFF]', u'', body)
    #converting to lower case
    body = body.lower()
    
    tokens = word_tokenize(body)
    if debug:
      print(tokens)
   
    return " ".join(tokens)


def parse_emails(folder_path, spam_label, corpus_file = None, csv_file = None):
    key_set = {}
    print("inside parsing emails")
    fields = ['spam', 'subject','sender','reply_to','num_inline_attach', 'num_attachment', 'urls','len_subject','subject_non_ascii','subject_contain_num','num_recipients','sender_replyto_same','header_contentType'
    ,'IP_in_url','Length_of_url','@_in_url','\\\ in url','- in domain_url','mailto: in url','resolver_url','body']
    corpus_fp = None
    if corpus_file is not None:
        #print(corpus_file)
        corpus_fp = open(corpus_file, "w")

    csv_fp = None
    if csv_file is not None:
        #print(csv_file)
        csv_fp = open(csv_file, "w")
    header_written = False
    rows = []
    for dirpath, dirnames, files in os.walk(folder_path, topdown=False):
        for file_name in files:
            print(file_name)
            with open(os.path.join(dirpath, file_name)) as fp:
                try:
                    row = {}
                    urls = {}
                    text_htm = ""
                    text_plain = ""
                    msg = email.message_from_file(fp)
                    num_inline = 0
                    num_attachment = 0
                    non_ascii_regex = re.compile('[@_!#$%^&*()<>?/\|}{~:]')
                    num_regex=re.compile('[0-9]')  

                    for part in msg.walk():
                        if not part.is_multipart():
                            
                            cont_disp = part.get_content_disposition()
                            if cont_disp is not None:
                                #print(f'cont_disp : {cont_disp}')
                                cont_disp_lc = cont_disp.lower()
                                if cont_disp_lc == 'attachment':
                                    num_attachment += 1
                                elif cont_disp_lc == 'inline':
                                    num_inline += 1

                            sub_ct = part.get_content_type().lower()
                            #extracting plain text from html
                            if(sub_ct == 'text/html' or sub_ct == 'text/plain'):
                                pl = part.get_payload()
                                soup = BeautifulSoup(pl, "html.parser")
                                isHtml = bool(soup.find())
                                
                                if isHtml:
                                  text = soup.get_text()
                                  try:
                                      #try to decode quoted printable encoding for both html and text
                                      for link in soup.find_all('a'):
                                        url = link.get('href') 
                                        if url is not None :
                                          url = qp.decodestring(url).decode()
                                          url = url.replace("3D","") 
                                          url = url.replace('"','')
                                          url = url.replace("'","")
                                          urls[url] = urls.get(url, 0) + 1
                                          # print(f'url : {url}')

                                      text_htm += " " + qp.decodestring(text).decode()
                                  except ValueError:
                                      #html does not contain quoted printable encoding
                                      text_htm += " " + text
                                      for link in soup.find_all('a'):
                                        url = link.get('href') 
                                        if url is not None :
                                          url = url.replace("3D","") 
                                          url = url.replace('"','')
                                          url = url.replace("'","")
                                          urls[url] = urls.get(url, 0) + 1
                                          # print(f'url (exception): {url}')
                                else:
                                  text_plain += " " + part.get_payload()
                            else :
                                pass
                    

                    #finding all url in plain text and plain text extracted from html
                    for url in re.findall('http[s]?://\S+', text_plain + " " + text_htm):
                        urls[url] = urls.get(url, 0) + 1
                        # print(f'plain text url : {url}')
                    #cleaning text
                    text_plain = clean_text(text_plain)
                    text_htm = clean_text(text_htm)
                    # checking if both text are same (discard html if its same as plain text, choosing cutoff at 85%)
                    seq = SequenceMatcher(None, text_plain, text_htm)
                    seq_ratio = seq.ratio()
                    # print(f"seq ratio : {seq_ratio}")
                    body = ""
                    if(seq_ratio > 0.85):
                        body += text_plain
                    else :
                        body += text_plain + " " + text_htm

                    if body is not None:
                       #print(len(body))
                       length_b=len(body)
                       row[fields[20]] = length_b
                       #print()

                    if corpus_fp is not None:
                        corpus_fp.write(body + "\n")
                        corpus_fp.flush()
                    
                    #writing to csv
                    if csv_fp is not None:

                        row[fields[0]] = spam_label
                        subj = msg.get('subject', None)
                        if subj is None:
                            subj = msg.get('SUBJECT', None)
                        if subj is None:
                            subj = msg.get('Subject', "")

                        row[fields[1]] = subj
                        sender = msg.get('From', None)
                        if sender is None:
                            sender = msg.get('FROM', None)
                        if sender is None:
                            sender = msg.get('from', None)

                        row[fields[2]] = " ".join(re.findall('[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+', sender)) if sender is not None else sender

                        replyTo = None
                        replyTo = msg.get('Reply-To', None)
                        if replyTo is None:
                            replyTo = msg.get('Reply-to', None)
                        if replyTo is None:
                            replyTo = msg.get('Replyto', None)
                        if replyTo is None:
                            replyTo = msg.get('reply-to', None)
                        # print(replyTo)

                        row[fields[3]] = " ".join(re.findall('[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+', replyTo)) if replyTo is not None else replyTo

                        row[fields[4]] = num_inline
                        row[fields[5]] = num_attachment
                        keys = urls.keys()
                        if len(keys) > 0:
                            final_url=parse_url(keys)
                            #print("inside",final_url)
                            row[fields[6]] = " ".join(keys) if len(keys) > 0 else None
                            row[fields[13]] = final_url[0]
                            row[fields[14]] = final_url[1]
                            row[fields[15]] = final_url[2]
                            row[fields[16]] = final_url[3]
                            row[fields[17]] = final_url[4]
                            row[fields[18]] = final_url[5]
                            row[fields[19]] = final_url[6]


                        row[fields[7]] = len(subj)
                        if non_ascii_regex.search(subj) is  None:
                             row[fields[8]] = False
                        else:
                             row[fields[8]] = True
                        if num_regex.search(subj) is None:
                             row[fields[9]] = False
                        else:
                             row[fields[9]] = True  
                        
                        recipients=msg.get('To',None)
                        recipients_array=None
                        if recipients is None:
                            row[fields[10]]=0
                        else:
                            recipients_array=recipients.split(',')
                            row[fields[10]]=len(recipients_array)

                        if sender == replyTo:
                            sender_replyto_same=True
                        else:
                            sender_replyto_same=False

                        row[fields[11]]=sender_replyto_same
                        contentType=None
                        if contentType is None:
                            contentType=msg.get('Content-Type',None)
                        if contentType is None:
                            contentType=msg.get('Content-type',None)
                        if contentType is not None:  
                          if ';' in contentType:
                            contentType=contentType.split(';')[0]    
                        #print(contentType)    
                        row[fields[12]]=contentType
                        rows.append(row)

                except UnicodeDecodeError:
                    pass  
    
    if corpus_fp is not None:
        corpus_fp.close()

    if csv_fp is not None:
        csvwriter = csv.DictWriter(csv_fp, fieldnames = fields)
        if not header_written:
          csvwriter.writeheader()
          # writing the fields  
          csvwriter.writerows(rows)
        csv_fp.close() 

    print(key_set)

"""CSV for Ham Emails"""

parse_emails('/content/drive/MyDrive/hard_ham_eml', 0, csv_file='email_data_eh.csv')
parse_emails('/content/drive/MyDrive/easy_ham_eml', 0, csv_file='email_data_eh.csv')

"""# CSV for Spam Emails

"""

parse_emails('/content/drive/MyDrive/spam2_ham_eml', 1, csv_file='email_data_sp.csv')

"""## Email Content Types
- multipart/alternative (same email content from poor format (like text) to rich format(eg html) in multiple parts)
- multipart/mixed (with attachments)
- multipart/related (embedded images with text like base64 img etc)
- multipart/signed ()
- multipart/report
- text/plain
- text/html


"""

df_spam = pd.read_csv('email_data_sp.csv')
#display(df_spam)
df_spam.describe()

df_ham = pd.read_csv('email_data_eh.csv')
#display(df_ham)
df_ham.describe()

import sklearn.model_selection as model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn import preprocessing
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

l = preprocessing.LabelEncoder()
df=df_ham.append(df_spam)
df['subject_non_ascii']=l.fit_transform(df['subject_non_ascii'])
df['subject_contain_num']=l.fit_transform(df['subject_contain_num'])
df['num_recipients']=l.fit_transform(df['num_recipients'])
df['sender_replyto_same']=l.fit_transform(df['sender_replyto_same'])
df['Length_of_url']=l.fit_transform(df['Length_of_url'])
df['IP_in_url']=l.fit_transform(df['IP_in_url'])
df['@_in_url']=l.fit_transform(df['@_in_url'])
df['\\\ in url']=l.fit_transform(df['\\\ in url'])
df['- in domain_url']=l.fit_transform(df['- in domain_url'])
df['mailto: in url']=l.fit_transform(df['mailto: in url'])
df['resolver_url']=l.fit_transform(df['resolver_url'])
df['header_contentType']=l.fit_transform(df['header_contentType'].astype(str))
df['body']=l.fit_transform(df['body'])

new1=df[['num_inline_attach', 'num_attachment','len_subject','subject_non_ascii','subject_contain_num','num_recipients','sender_replyto_same'
    ,'IP_in_url','Length_of_url','@_in_url','\\\ in url','- in domain_url','mailto: in url','resolver_url','header_contentType','body']]

fig = plt.figure(figsize=(15, 15))

for i in range(1, new1.shape[1]):
    plt.subplot(5, 3, i)
    f = plt.gca()
    f.axes.get_yaxis().set_visible(False)
    f.set_title(new1.columns.values[i])
    vals = np.size(new1.iloc[:, i].unique())
    plt.hist(new1.iloc[:, i], bins=vals, color='#3F5D7D')

import seaborn as sns

corr_matrix = new1.corr()
#corr_matrix
f, ax = plt.subplots(figsize=(11, 15))

heatmap = sns.heatmap(corr_matrix,
                      square = True,
                      linewidths = .5,
                      cmap = 'coolwarm',
                      cbar_kws = {'shrink': .4,
                                'ticks' : [-1, -.5, 0, 0.5, 1]},
                      vmin = -1,
                      vmax = 1,
                      annot = True,
                      annot_kws = {"size": 12})

#add the column names as labels
ax.set_yticklabels(corr_matrix.columns, rotation = 0)
ax.set_xticklabels(corr_matrix.columns)

sns.set_style({'xtick.bottom': True}, {'ytick.left': True})

"""Missing values


*  Email without URL should be given 'NO URL' value for URL 
*  If there is no URL in the email , attribute related to URL's value is considered 'False' i.e. legitimate

attribute
"""

#df["urls"].fillna("No URL", inplace = True)
#df[['IP_in_url','Length_of_url','@_in_url','\\\ in url','- in domain_url','mailto: in url','resolver_url']].fillna(False,inplace=True)
data =df[['spam','num_inline_attach', 'num_attachment','len_subject','subject_non_ascii','subject_contain_num','num_recipients','sender_replyto_same'
    ,'IP_in_url','Length_of_url','@_in_url','\\\ in url','- in domain_url','mailto: in url','resolver_url','header_contentType','body']]
#data.dropna()
#X=df[['num_inline_attach', 'num_attachment','len_subject','subject_non_ascii','subject_contain_num','num_recipients','sender_replyto_same'
#    ,'IP_in_url','Length_of_url','@_in_url','\\\ in url','- in domain_url','mailto: in url','resolver_url']]

X=df[['num_inline_attach', 'num_attachment','len_subject','subject_non_ascii','subject_contain_num','num_recipients','sender_replyto_same'
    ,'IP_in_url','Length_of_url','@_in_url','\\\ in url','- in domain_url','mailto: in url','header_contentType','body']]

   
y=df[['spam']]
print(X.shape)
print(y.shape)
X

def plot_roc_curve(fpr, tpr):
    plt.plot(fpr, tpr, color='orange', label='ROC')
    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()

#Logistic regression model
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from numpy import mean
from numpy import std
from sklearn import metrics
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_confusion_matrix


X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.7,test_size=0.3, random_state=101)

logisticRegr = LogisticRegression(max_iter=7600)
logisticRegr.fit(X_train, y_train.values.ravel())
y_pred = logisticRegr.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("F1 score:",metrics.f1_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred))
print("recall:",metrics.recall_score(y_test, y_pred))

probs = logisticRegr.predict_proba(X_test)
probs = probs[:, 1]
auc = roc_auc_score(y_test, probs)
print('AUC: %.2f' % auc)
fpr, tpr, thresholds = roc_curve(y_test, probs)
print("fpr: ",mean(fpr))
print("tpr: ",mean(tpr))
plot_roc_curve(fpr, tpr)
matrix = plot_confusion_matrix(logisticRegr, X_test, y_test,
                                 cmap=plt.cm.Blues,
                                 normalize='true')
plt.title('Confusion matrix for our classifier')
plt.show(matrix)
plt.show()

#SVM model
from sklearn import svm
from sklearn import metrics
from sklearn.metrics import plot_confusion_matrix


clf = svm.SVC(kernel='linear',probability=True) 
#clf.fit(X_train, y_train.values.ravel())
y_pred = clf.predict(X_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("F1 score:",metrics.f1_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred))
print("Recall:",metrics.recall_score(y_test, y_pred))

probs = clf.predict_proba(X_test)
probs = probs[:, 1]
auc = roc_auc_score(y_test, probs)
print('AUC: %.2f' % auc)
fpr, tpr, thresholds = roc_curve(y_test, probs)
print("fpr: ",mean(fpr))
print("tpr: ",mean(tpr))
#plot_roc_curve(fpr, tpr)
#matrix = plot_confusion_matrix(clf, X_test, y_test,cmap=plt.cm.Blues,normalize='true')
#plt.title('Confusion matrix for our classifier')
#plt.show(matrix)
#plt.show()

#naive bayes model
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn import preprocessing
from sklearn.metrics import plot_confusion_matrix



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,random_state=109) 
gnb = GaussianNB()
gnb.fit(X_train, y_train.values.ravel())
y_pred = gnb.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("F1 score:",metrics.f1_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred))
print("recall:",metrics.recall_score(y_test, y_pred))

probs = gnb.predict_proba(X_test)
probs = probs[:, 1]
auc = roc_auc_score(y_test, probs)
print('AUC: %.2f' % auc)
fpr, tpr, thresholds = roc_curve(y_test, probs)
print("fpr: ",mean(fpr))
print("tpr: ",mean(tpr))
plot_roc_curve(fpr, tpr)
matrix = plot_confusion_matrix(gnb, X_test, y_test,
                                 cmap=plt.cm.Blues,
                                 normalize='true')
plt.title('Confusion matrix for our classifier')
plt.show(matrix)
plt.show()

#KNN model
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import plot_confusion_matrix


scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
classifier = KNeighborsClassifier(n_neighbors=5)
classifier.fit(X_train, y_train.values.ravel())
y_pred = classifier.predict(X_test)
classifier.score(X_test, y_test)
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("F1 score:",metrics.f1_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred))
print("Recall:",metrics.recall_score(y_test, y_pred))

probs = classifier.predict_proba(X_test)
probs = probs[:, 1]
auc = roc_auc_score(y_test, probs)
print('AUC: %.2f' % auc)
fpr, tpr, thresholds = roc_curve(y_test, probs)
print("fpr: ",mean(fpr))
print("tpr: ",mean(tpr))
plot_roc_curve(fpr, tpr)
matrix = plot_confusion_matrix(classifier, X_test, y_test,
                                 cmap=plt.cm.Blues,
                                 normalize='true')
plt.title('Confusion matrix for our classifier')
plt.show(matrix)
plt.show()

# Random forest classifier

from sklearn.ensemble import RandomForestClassifier
import pandas as pd
from sklearn import metrics
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import plot_confusion_matrix


clf=RandomForestClassifier(n_estimators=100)
clf.fit(X_train,y_train.values.ravel())
y_pred=clf.predict(X_test)

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("F1 score:",metrics.f1_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred))
print("Recall:",metrics.recall_score(y_test, y_pred))

probs = clf.predict_proba(X_test)
probs = probs[:, 1]
auc = roc_auc_score(y_test, probs)
print('AUC: %.2f' % auc)
fpr, tpr, thresholds = roc_curve(y_test, probs)
print("fpr: ",mean(fpr))
print("tpr: ",mean(tpr))
plot_roc_curve(fpr, tpr)
matrix = plot_confusion_matrix(clf, X_test, y_test,
                                 cmap=plt.cm.Blues,
                                 normalize='true')
plt.title('Confusion matrix for our classifier')
plt.show(matrix)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
# Creating a bar plot
feature_imp = pd.Series(clf.feature_importances_).sort_values(ascending=False)
feature_imp
sns.barplot(x=feature_imp, y=feature_imp.index)
# Add labels to your graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.legend()
plt.show()